{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eeaae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pickle # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# importing library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
    "from sklearn.metrics import classification_report # for generating a classification report of model\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from keras.layers import Dropout, Activation, Flatten, Convolution1D, Dropout, Reshape\n",
    "from keras.layers import Dense # importing dense layer\n",
    "from keras.models import Sequential #importing Sequential layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bce03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define learning rate\n",
    "lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba51714",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(gpu, \"\\n\")\n",
    "else:\n",
    "  print(\"No GPU device found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619bc59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfeb927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "path = r'H:/Datasets/CICIDS-2017/MachineLearningCVE' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, encoding='cp1252', index_col=None, header=0, low_memory=False)\n",
    "    li.append(df)\n",
    "    print(\"Read Completed for \", filename)\n",
    "    \n",
    "print(\"Reading Finished\")\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "df = df.rename(columns={' Label': 'Label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe9114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the labels\n",
    "def clean_label(label):\n",
    "    # Replace non-ASCII characters with a hyphen\n",
    "    label = re.sub(r'[^\\x00-\\x7F]+', '-', label)\n",
    "    return label.strip()  # Optional: Strip leading and trailing whitespace\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['Label'] = df['Label'].apply(clean_label)\n",
    "\n",
    "\n",
    "df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b94424",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144de56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "def expand_categories(values):\n",
    "    result = []\n",
    "    s = values.value_counts()\n",
    "    t = float(len(values))\n",
    "    for v in s.index:\n",
    "        result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
    "    return \"[{}]\".format(\",\".join(result))\n",
    "        \n",
    "def analyze(df):\n",
    "    print()\n",
    "    cols = df.columns.values\n",
    "    total = float(len(df))\n",
    "\n",
    "    print(\"{} rows\".format(int(total)))\n",
    "    for col in cols:\n",
    "        uniques = df[col].unique()\n",
    "        unique_count = len(uniques)\n",
    "        if unique_count>100:\n",
    "            print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)/total)*100)))\n",
    "        else:\n",
    "            print(\"** {}:{}\".format(col,expand_categories(df[col])))\n",
    "            expand_categories(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "class_distribution = df['Label'].value_counts()\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Data points per Class')\n",
    "plt.title('Distribution of CICIDS2017 Training Data')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('CIC-IDS2017_Data_Distribution.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61448dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Cleaning Data set for Duplicate\n",
    "sorted_ds = np.argsort(-class_distribution.values)\n",
    "for i in sorted_ds:\n",
    "    print('Number of data points in class', class_distribution.index[i],':', class_distribution.values[i], \n",
    "          '(', np.round((class_distribution.values[i]/df.shape[0]*100), 3), '%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573844ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sorted_ds\n",
    "\n",
    "#drop na values and reset index\n",
    "data_clean = df.dropna().reset_index()\n",
    "\n",
    "# Checkng for DUPLICATE values\n",
    "data_clean.drop_duplicates(keep='first', inplace = True)\n",
    "\n",
    "data_clean['Label'].value_counts()\n",
    "\n",
    "print(\"Read {} rows.\".format(len(data_clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59117a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f264ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "class_distribution = data_clean['Label'].value_counts()\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Data points per Class')\n",
    "plt.title('Distribution of Cleaned CICIDS2017 Training Data')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea136e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Cleaning Data set for Duplicate\n",
    "sorted_ds = np.argsort(-class_distribution.values)\n",
    "for i in sorted_ds:\n",
    "    print('Number of data points in class', class_distribution.index[i],':', class_distribution.values[i], \n",
    "          '(', np.round((class_distribution.values[i]/data_clean.shape[0]*100), 3), '%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "#drop na values and reset index\n",
    "data_clean = data_clean.dropna().reset_index()\n",
    "\n",
    "# label encoding\n",
    "labelencoder = LabelEncoder()\n",
    "data_clean['Label'] = labelencoder.fit_transform(data_clean['Label'])\n",
    "\n",
    "data_clean['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f611d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aad1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = data_clean.to_numpy(dtype=\"float32\")\n",
    "\n",
    "#drop inf values\n",
    "data_np = data_np[~np.isinf(data_np).any(axis=1)]\n",
    "\n",
    "# Separate features (X) and target (Y)\n",
    "X = data_np[:, :-1]  # All columns except the last one\n",
    "enc = OneHotEncoder()\n",
    "Y = enc.fit_transform(data_np[:, -1:]).toarray()  # One-hot encode the last column (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7bfb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y, test_size=0.25, random_state=2, shuffle=True)\n",
    "\n",
    "_features = X.shape[1]\n",
    "n_classes = Y.shape[1]\n",
    "\n",
    "print('X.shape = ',X.shape)\n",
    "print('Y.shape = ',Y.shape)\n",
    "print('X_train.shape = ',X_train.shape)\n",
    "print('y_train.shape = ', Y_train.shape)\n",
    "print('X_test.shape = ', X_test.shape)\n",
    "print('y_test.shape = ',Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f322b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X Shape: \", X.shape)\n",
    "print(\"Y Shape: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.iloc[:,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3afe155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data.iloc[:,0] = label_encoder.fit_transform(data.iloc[:,0]).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a220b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(15,10)})\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0edbb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "879f0d8b",
   "metadata": {},
   "source": [
    "# IO-DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, MaxPooling1D\n",
    "from sklearn import metrics\n",
    "from hyperopt import fmin, hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.plotting import main_plot_history, main_plot_vars\n",
    "import uuid\n",
    "import gc\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], _features, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], _features, 1).astype('float32')\n",
    "\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(filters=128, kernel_size=6, activation='relu', input_shape=(_features, 1)))\n",
    "model.add(Convolution1D(filters=96, kernel_size=6, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=1, padding='same'))\n",
    "\n",
    "\n",
    "model.add(Convolution1D(filters=64, kernel_size=6, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=1, padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(96, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary() \n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "   \n",
    "history = model.fit(X_train, Y_train,\n",
    "                              batch_size=128,\n",
    "                              epochs=10,\n",
    "                              verbose=True, \n",
    "                              validation_data=(X_test, Y_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a49b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict_classes(X_test)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "balanced_score = metrics.balanced_accuracy_score(np.argmax(Y_test, axis=1), y_pred) * 100\n",
    "    \n",
    "best_loss = np.amin(history.history['val_loss']) \n",
    "print('Best loss: {}'.format(best_loss))\n",
    "print('Balanced Acc loss: {}'.format(balanced_score))\n",
    "    \n",
    "    \n",
    "import tensorflow.keras.backend as K\n",
    "print('Learning Rate - ')\n",
    "print(K.eval(model.optimizer.lr)) \n",
    "print('==================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f233b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "\n",
    "escore = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "pred = model.predict(X_test)\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(Y_test,axis=1)\n",
    "\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "rscore = recall_score(y_eval, pred, average='weighted')\n",
    "ascore = precision_score(y_eval, pred, average='weighted')\n",
    "f1score= f1_score(y_eval, pred, average='weighted') #F1 = 2 * (precision * recall) / (precision + recall) for manual\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_eval)\n",
    "y_eval = lb.transform(y_eval)\n",
    "pred = lb.transform(pred)\n",
    "roc_score = roc_auc_score(y_eval, pred)\n",
    "\n",
    "print('Completed')\n",
    "print('Time taken:',dt.datetime.now()-start)\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "# Given data\n",
    "validation_time_total = inference_time  # Total validation time in seconds\n",
    "validation_samples = X_test.shape  # Number of validation samples\n",
    "batch_size = 128  # Batch size\n",
    "\n",
    "# Calculate the number of batches\n",
    "#num_batches = np.ceil(validation_samples / batch_size)\n",
    "num_batches = np.ceil(validation_samples[0] / batch_size)\n",
    "\n",
    "# Calculate time per batch during inference\n",
    "time_per_batch = validation_time_total / num_batches\n",
    "\n",
    "# Calculate detection time per sample\n",
    "detection_time_per_sample = time_per_batch / batch_size\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of Batches: {num_batches}\")\n",
    "print(f\"Time per Batch (Inference): {time_per_batch:.6f} seconds\")\n",
    "print(f\"Detection Time per Sample: {detection_time_per_sample:.6f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Time for fitting: {:.2f} seconds\".format(inference_time))\n",
    "\n",
    "print(\"Validation score: {}\".format(score))\n",
    "print(\"Evaluation score: {}\".format(escore))\n",
    "print(\"Recall score: {}\".format(rscore))\n",
    "print(\"Precision score: {}\".format(ascore))\n",
    "print(\"F1 Measure score: {}\".format(f1score))\n",
    "print(\"ROC-AUC score: {}\".format(roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for training and validation loss\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc61f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_test = Y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sn\n",
    "\n",
    "confMat = confusion_matrix(y_test, pred)\n",
    "confMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7cb93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, pred)\n",
    "cm_df = pd.DataFrame(cf_matrix)\n",
    "\n",
    "labels = ['Benign','DoS Hulk','Port Scan','DDoS','DoS Golden Eye','FTP Patator','SSH Patator','DoS slowloris',\n",
    "          'DoS slowhttptest','Bot','Web Attack - Brute Force','Web Attack - XSS','Infiltration','Web Attack - Sql Injection',\n",
    "         'Heartbleed']\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(cm_df, annot=True, annot_kws={\"size\":12}, fmt='g', xticklabels=labels, yticklabels=labels, cmap='Blues')\n",
    "\n",
    "#sn.heatmap(cm_df, annot=True, annot_kws={\"size\":12}, fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "    \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1973d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f34db4fd",
   "metadata": {},
   "source": [
    "# IO-DCNN - Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6328fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, MaxPooling1D\n",
    "from sklearn import metrics\n",
    "from hyperopt import fmin, hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.plotting import main_plot_history, main_plot_vars\n",
    "import uuid\n",
    "import gc\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], _features, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], _features, 1).astype('float32')\n",
    "\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(filters=128, kernel_size=6, activation='relu', input_shape=(_features, 1)))\n",
    "model.add(Convolution1D(filters=96, kernel_size=6, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=1, padding='same'))\n",
    "\n",
    "\n",
    "model.add(Convolution1D(filters=64, kernel_size=6, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=1, padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(96, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary() \n",
    "\n",
    "opt = keras.optimizers.Adagrad(learning_rate=lr)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "history = model.fit(X_train, Y_train,\n",
    "                              batch_size=128,\n",
    "                              epochs=10,\n",
    "                              verbose=True, \n",
    "                              validation_data=(X_test, Y_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict_classes(X_test)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "balanced_score = metrics.balanced_accuracy_score(np.argmax(Y_test, axis=1), y_pred) * 100\n",
    "    \n",
    "best_loss = np.amin(history.history['val_loss']) \n",
    "print('Best loss: {}'.format(best_loss))\n",
    "print('Balanced Acc loss: {}'.format(balanced_score))\n",
    "    \n",
    "    \n",
    "import tensorflow.keras.backend as K\n",
    "print('Learning Rate - ')\n",
    "print(K.eval(model.optimizer.lr)) \n",
    "print('==================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "\n",
    "escore = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "pred = model.predict(X_test)\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(Y_test,axis=1)\n",
    "\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "rscore = recall_score(y_eval, pred, average='weighted')\n",
    "ascore = precision_score(y_eval, pred, average='weighted')\n",
    "f1score= f1_score(y_eval, pred, average='weighted') #F1 = 2 * (precision * recall) / (precision + recall) for manual\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_eval)\n",
    "y_eval = lb.transform(y_eval)\n",
    "pred = lb.transform(pred)\n",
    "roc_score = roc_auc_score(y_eval, pred)\n",
    "\n",
    "print('Completed')\n",
    "print('Time taken:',dt.datetime.now()-start)\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "# Given data\n",
    "validation_time_total = inference_time  # Total validation time in seconds\n",
    "validation_samples = X_test.shape  # Number of validation samples\n",
    "batch_size = 128  # Batch size\n",
    "\n",
    "# Calculate the number of batches\n",
    "#num_batches = np.ceil(validation_samples / batch_size)\n",
    "num_batches = np.ceil(validation_samples[0] / batch_size)\n",
    "\n",
    "# Calculate time per batch during inference\n",
    "time_per_batch = validation_time_total / num_batches\n",
    "\n",
    "# Calculate detection time per sample\n",
    "detection_time_per_sample = time_per_batch / batch_size\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of Batches: {num_batches}\")\n",
    "print(f\"Time per Batch (Inference): {time_per_batch:.6f} seconds\")\n",
    "print(f\"Detection Time per Sample: {detection_time_per_sample:.6f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Time for fitting: {:.2f} seconds\".format(inference_time))\n",
    "\n",
    "print(\"Validation score: {}\".format(score))\n",
    "print(\"Evaluation score: {}\".format(escore))\n",
    "print(\"Recall score: {}\".format(rscore))\n",
    "print(\"Precision score: {}\".format(ascore))\n",
    "print(\"F1 Measure score: {}\".format(f1score))\n",
    "print(\"ROC-AUC score: {}\".format(roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_test = Y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sn\n",
    "\n",
    "confMat = confusion_matrix(y_test, pred)\n",
    "confMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a1906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "328c142a",
   "metadata": {},
   "source": [
    "# IO-DCNN - Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, MaxPooling1D\n",
    "from sklearn import metrics\n",
    "from hyperopt import fmin, hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.plotting import main_plot_history, main_plot_vars\n",
    "import uuid\n",
    "import gc\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], _features, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], _features, 1).astype('float32')\n",
    "\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(filters=128, kernel_size=6, activation='relu', input_shape=(_features, 1)))\n",
    "model.add(Convolution1D(filters=96, kernel_size=6, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=1, padding='same'))\n",
    "\n",
    "\n",
    "model.add(Convolution1D(filters=64, kernel_size=6, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=1, padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(96, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary() \n",
    "\n",
    "opt = keras.optimizers.Adamax(learning_rate=lr)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "history = model.fit(X_train, Y_train,\n",
    "                              batch_size=128,\n",
    "                              epochs=10,\n",
    "                              verbose=True, \n",
    "                              validation_data=(X_test, Y_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict_classes(X_test)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "balanced_score = metrics.balanced_accuracy_score(np.argmax(Y_test, axis=1), y_pred) * 100\n",
    "    \n",
    "best_loss = np.amin(history.history['val_loss']) \n",
    "print('Best loss: {}'.format(best_loss))\n",
    "print('Balanced Acc loss: {}'.format(balanced_score))\n",
    "    \n",
    "    \n",
    "import tensorflow.keras.backend as K\n",
    "print('Learning Rate - ')\n",
    "print(K.eval(model.optimizer.lr)) \n",
    "print('==================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a9f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "\n",
    "escore = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "pred = model.predict(X_test)\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(Y_test,axis=1)\n",
    "\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "rscore = recall_score(y_eval, pred, average='weighted')\n",
    "ascore = precision_score(y_eval, pred, average='weighted')\n",
    "f1score= f1_score(y_eval, pred, average='weighted') #F1 = 2 * (precision * recall) / (precision + recall) for manual\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_eval)\n",
    "y_eval = lb.transform(y_eval)\n",
    "pred = lb.transform(pred)\n",
    "roc_score = roc_auc_score(y_eval, pred)\n",
    "\n",
    "print('Completed')\n",
    "print('Time taken:',dt.datetime.now()-start)\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "# Given data\n",
    "validation_time_total = inference_time  # Total validation time in seconds\n",
    "validation_samples = X_test.shape  # Number of validation samples\n",
    "batch_size = 128  # Batch size\n",
    "\n",
    "# Calculate the number of batches\n",
    "#num_batches = np.ceil(validation_samples / batch_size)\n",
    "num_batches = np.ceil(validation_samples[0] / batch_size)\n",
    "\n",
    "# Calculate time per batch during inference\n",
    "time_per_batch = validation_time_total / num_batches\n",
    "\n",
    "# Calculate detection time per sample\n",
    "detection_time_per_sample = time_per_batch / batch_size\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of Batches: {num_batches}\")\n",
    "print(f\"Time per Batch (Inference): {time_per_batch:.6f} seconds\")\n",
    "print(f\"Detection Time per Sample: {detection_time_per_sample:.6f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Time for fitting: {:.2f} seconds\".format(inference_time))\n",
    "\n",
    "print(\"Validation score: {}\".format(score))\n",
    "print(\"Evaluation score: {}\".format(escore))\n",
    "print(\"Recall score: {}\".format(rscore))\n",
    "print(\"Precision score: {}\".format(ascore))\n",
    "print(\"F1 Measure score: {}\".format(f1score))\n",
    "print(\"ROC-AUC score: {}\".format(roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_test = Y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef05fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sn\n",
    "\n",
    "confMat = confusion_matrix(y_test, pred)\n",
    "confMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e861e35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c16d2b",
   "metadata": {},
   "source": [
    "# IO-DCNN - Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd32521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, MaxPooling1D\n",
    "from sklearn import metrics\n",
    "from hyperopt import fmin, hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.plotting import main_plot_history, main_plot_vars\n",
    "import uuid\n",
    "import gc\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], _features, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], _features, 1).astype('float32')\n",
    "\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(filters=128, kernel_size=6, activation='relu', input_shape=(_features, 1)))\n",
    "model.add(Convolution1D(filters=96, kernel_size=6, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=1, padding='same'))\n",
    "\n",
    "\n",
    "model.add(Convolution1D(filters=64, kernel_size=6, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=1, padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(96, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary() \n",
    "\n",
    "opt = keras.optimizers.Nadam(learning_rate=lr)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "history = model.fit(X_train, Y_train,\n",
    "                              batch_size=128,\n",
    "                              epochs=10,\n",
    "                              verbose=True, \n",
    "                              validation_data=(X_test, Y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ab835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict_classes(X_test)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "balanced_score = metrics.balanced_accuracy_score(np.argmax(Y_test, axis=1), y_pred) * 100\n",
    "    \n",
    "best_loss = np.amin(history.history['val_loss']) \n",
    "print('Best loss: {}'.format(best_loss))\n",
    "print('Balanced Acc loss: {}'.format(balanced_score))\n",
    "    \n",
    "    \n",
    "import tensorflow.keras.backend as K\n",
    "print('Learning Rate - ')\n",
    "print(K.eval(model.optimizer.lr)) \n",
    "print('==================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "\n",
    "escore = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "pred = model.predict(X_test)\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(Y_test,axis=1)\n",
    "\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "rscore = recall_score(y_eval, pred, average='weighted')\n",
    "ascore = precision_score(y_eval, pred, average='weighted')\n",
    "f1score= f1_score(y_eval, pred, average='weighted') #F1 = 2 * (precision * recall) / (precision + recall) for manual\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_eval)\n",
    "y_eval = lb.transform(y_eval)\n",
    "pred = lb.transform(pred)\n",
    "roc_score = roc_auc_score(y_eval, pred)\n",
    "\n",
    "print('Completed')\n",
    "print('Time taken:',dt.datetime.now()-start)\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "# Given data\n",
    "validation_time_total = inference_time  # Total validation time in seconds\n",
    "validation_samples = X_test.shape  # Number of validation samples\n",
    "batch_size = 128  # Batch size\n",
    "\n",
    "# Calculate the number of batches\n",
    "#num_batches = np.ceil(validation_samples / batch_size)\n",
    "num_batches = np.ceil(validation_samples[0] / batch_size)\n",
    "\n",
    "# Calculate time per batch during inference\n",
    "time_per_batch = validation_time_total / num_batches\n",
    "\n",
    "# Calculate detection time per sample\n",
    "detection_time_per_sample = time_per_batch / batch_size\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of Batches: {num_batches}\")\n",
    "print(f\"Time per Batch (Inference): {time_per_batch:.6f} seconds\")\n",
    "print(f\"Detection Time per Sample: {detection_time_per_sample:.6f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Time for fitting: {:.2f} seconds\".format(inference_time))\n",
    "\n",
    "print(\"Validation score: {}\".format(score))\n",
    "print(\"Evaluation score: {}\".format(escore))\n",
    "print(\"Recall score: {}\".format(rscore))\n",
    "print(\"Precision score: {}\".format(ascore))\n",
    "print(\"F1 Measure score: {}\".format(f1score))\n",
    "print(\"ROC-AUC score: {}\".format(roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_test = Y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sn\n",
    "\n",
    "confMat = confusion_matrix(y_test, pred)\n",
    "confMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d120550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fc7ec34",
   "metadata": {},
   "source": [
    "# IO-DCNN - SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, MaxPooling1D\n",
    "from sklearn import metrics\n",
    "from hyperopt import fmin, hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.plotting import main_plot_history, main_plot_vars\n",
    "import uuid\n",
    "import gc\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], _features, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], _features, 1).astype('float32')\n",
    "\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(filters=128, kernel_size=6, activation='relu', input_shape=(_features, 1)))\n",
    "model.add(Convolution1D(filters=96, kernel_size=6, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=1, padding='same'))\n",
    "\n",
    "\n",
    "model.add(Convolution1D(filters=64, kernel_size=6, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=1, padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(96, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary() \n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=lr)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "history = model.fit(X_train, Y_train,\n",
    "                              batch_size=128,\n",
    "                              epochs=10,\n",
    "                              verbose=True, \n",
    "                              validation_data=(X_test, Y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict_classes(X_test)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "balanced_score = metrics.balanced_accuracy_score(np.argmax(Y_test, axis=1), y_pred) * 100\n",
    "    \n",
    "best_loss = np.amin(history.history['val_loss']) \n",
    "print('Best loss: {}'.format(best_loss))\n",
    "print('Balanced Acc loss: {}'.format(balanced_score))\n",
    "    \n",
    "    \n",
    "import tensorflow.keras.backend as K\n",
    "print('Learning Rate - ')\n",
    "print(K.eval(model.optimizer.lr)) \n",
    "print('==================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "\n",
    "escore = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "pred = model.predict(X_test)\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(Y_test,axis=1)\n",
    "\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "rscore = recall_score(y_eval, pred, average='weighted')\n",
    "ascore = precision_score(y_eval, pred, average='weighted')\n",
    "f1score= f1_score(y_eval, pred, average='weighted') #F1 = 2 * (precision * recall) / (precision + recall) for manual\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_eval)\n",
    "y_eval = lb.transform(y_eval)\n",
    "pred = lb.transform(pred)\n",
    "roc_score = roc_auc_score(y_eval, pred)\n",
    "\n",
    "print('Completed')\n",
    "print('Time taken:',dt.datetime.now()-start)\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "# Given data\n",
    "validation_time_total = inference_time  # Total validation time in seconds\n",
    "validation_samples = X_test.shape  # Number of validation samples\n",
    "batch_size = 128  # Batch size\n",
    "\n",
    "# Calculate the number of batches\n",
    "#num_batches = np.ceil(validation_samples / batch_size)\n",
    "num_batches = np.ceil(validation_samples[0] / batch_size)\n",
    "\n",
    "# Calculate time per batch during inference\n",
    "time_per_batch = validation_time_total / num_batches\n",
    "\n",
    "# Calculate detection time per sample\n",
    "detection_time_per_sample = time_per_batch / batch_size\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of Batches: {num_batches}\")\n",
    "print(f\"Time per Batch (Inference): {time_per_batch:.6f} seconds\")\n",
    "print(f\"Detection Time per Sample: {detection_time_per_sample:.6f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Time for fitting: {:.2f} seconds\".format(inference_time))\n",
    "\n",
    "print(\"Validation score: {}\".format(score))\n",
    "print(\"Evaluation score: {}\".format(escore))\n",
    "print(\"Recall score: {}\".format(rscore))\n",
    "print(\"Precision score: {}\".format(ascore))\n",
    "print(\"F1 Measure score: {}\".format(f1score))\n",
    "print(\"ROC-AUC score: {}\".format(roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_test = Y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2bf9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sn\n",
    "\n",
    "confMat = confusion_matrix(y_test, pred)\n",
    "confMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db83016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
